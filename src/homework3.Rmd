---
title: "DATA 621 - HW3"
author: "Andrew Bowen, Glen Davis, Shoshana Farber, Joshua Forster, Charles Ugiagbe"
date: "2023-10-23"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)

library(tidyverse)
library(modelr)
library(DataExplorer)
library(correlationfunnel)
library(caret)
library(knitr)
library(confintr)
```

## Homework 3 - Logistic Regression

### Overview

In this homework assignment, you will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0).

Your objective is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. You will provide classifications and probabilities for the evaluation data set using your binary logistic regression model. You can only use the variables given to you (or, variables that you derive from the variables provided).

Below is a short description of the variables of interest in the data set:

|Column|Description|
|--|--|
|`zn`|proportion of residential land zoned for large lots (over 25000 square feet) (*predictor variable*)|
|`indus`|proportion of non-retail business acres per suburb (*predictor variable*)|
|`chas`|a dummy var. for whether the suburb borders the Charles River (1) or not (0) (*predictor variable*)|
|`nox`|nitrogen oxides concentration (parts per 10 million) (*predictor variable*)|
|`rm`|average number of rooms per dwelling (*predictor variable*)|
|`age`|proportion of owner-occupied units built prior to 1940 (*predictor variable*)
|`dis`|weighted mean of distances to five Boston employment centers (*predictor variable*)
|`rad`|index of accessibility to radial highways (*predictor variable*)|
|`tax`|full-value property-tax rate per $10,000 (*predictor variable*)|
|`ptratio`|pupil-teacher ratio by town (*predictor variable*)|
|`lstat`|lower status of the population (percent) (*predictor variable*)|
|`medv`|median value of owner-occupied homes in $1000s (*predictor variable*)|
|**`target`**|**whether the crime rate is above the median crime rate (1) or not (0) (*response variable*)**|

### Data Loading

Let's load in the training dataset. 

```{r load-train-data}
train_df <- read.csv('https://raw.githubusercontent.com/ShanaFarber/businessAnalyticsDataMiningDATA621/main/data/crime-training-data_modified.csv')

head(train_df)  # preview data
```

### Data Exploration

```{r dim-data}
dim(train_df)
```

The dataset consists of 466 observations of 13 variables. There are 12 predictor variables and one response variable (`target`).

All of the columns in the dataset are numeric, but the predictor variable `chas` is a dummy variable, as is the response variable `target`. We recode them as factors. 

```{r }
train_df <- train_df |>
    mutate(chas = as.factor(chas), target = as.factor(target))

```

Let's take a look at the summary statistics for the variables in the dataset. 

```{r summary-stats}
summary(train_df)
```

We can see the mean, median, and interquartile ranges for each of the numeric variables in the dataset.

There do not appear to be any NA values in the dataset. Let's validate this. 

```{r}
sum(is.na(train_df))
```

There are no null values in the training dataset. 

Let's take a look at the distributions for the predictor variables. 

```{r plot-distributions}
par(mfrow=c(3,4))
par(mai=c(.3,.3,.3,.3))

variables <- names(train_df)
factors <- c("chas", "target")
for (i in 1:(length(variables)-1)) {
    if (variables[i] %in% factors){
        hist(as.numeric(train_df[[variables[i]]]), main = variables[i],
             col = "lightblue")
    }else{
        hist(train_df[[variables[i]]], main = variables[i], col = "lightblue")
    }
}
```

`rm` appears to be normally distributed. `zn`, `dis`, and `lstat` appear to be skewed to the right. All other columns seem to be multimodal. 

The distribution for `chas` appears degenerate at first glance. It looks like a near-zero variance predictor, which we can confirm using the `nearZeroVar` function from the `caret` package. 

```{r near_zero_var}
nzv <- nearZeroVar(train_df |> select(-target), saveMetrics = TRUE)
knitr::kable(nzv)
```

The percentage of unique values, `percentUnique`, in the sample for this predictor is less than the typical threshold of 10 percent, but there is a second criterion to consider: the `freqRatio`. This measures the frequency of the most common value (0 in this case) to the frequency of the second most common value (1 in this case). The `freqRatio` value for this predictor is less than the typical threshold of 19 (i.e. 95 occurrences of the most frequent value for every 5 occurrences of the second most frequent values). So it is not considered a near-zero variance predictor. Neither are any of the other predictors. 

We produce a correlation funnel to visualize the relationships between our predictors and our response. 

```{r correlation_funnel}
train_df_binarized <- train_df |>
    binarize(n_bins = 5, thresh_infreq = 0.01, name_infreq = "OTHER",
           one_hot = TRUE)
train_df_corr <- train_df_binarized |>
    correlate(target__1)
train_df_corr |>
    plot_correlation_funnel()

```

The correlation funnel plots the most important features towards the top. The variable `chas` is the least correlated to `target` by the Pearson Correlation coefficient. The correct coefficient to use to understand the strength of the relationship between two categorical variables is actually the $\phi$ coefficient. If one of the categorical variables had more than two categories, we would need to calculate $\phi$ using the formula for Cramer's V (also called Cramer's $\phi$). However, in the special case that both categorical variables are binary,  the value of the Cramer's V coefficient will actually be equal to the value of the Pearson Correlation coefficient. So either formula actually results in the same value for $\phi$. We prove this below.

```{r correlation_coefficients}
cramersv <- round(cramersv(train_df |> select(all_of(factors))), 5)
pearson <- round(cor(as.numeric(train_df$chas), as.numeric(train_df$target), method = "pearson"), 5)
(cramersv == pearson)

```

The value for $\phi$ is `r cramersv` regardless of the formula used to calculate it, and this very low value indicates very little correlation between `chas` and `target`. 